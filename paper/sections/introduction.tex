\section{Introduction}

Continuous monitoring of the Earth's surface is important for applications such as crop monitoring, land cover classification, and environmental management. Optical satellite imagery provides rich spectral information, but cloud contamination limits its usability. Synthetic Aperture Radar (SAR) can penetrate clouds and provide structural information under all weather conditions. However, SAR and optical images capture different physical properties, making the reconstruction of cloud-free optical imagery from SAR a nontrivial problem.

Earlier approaches used signal processing techniques such as sparse representation and multi-temporal dictionary learning. These methods are limited under dense clouds or highly dynamic surfaces. Deep learning approaches, including conditional GANs, diffusion models, and transformer-based networks, improve reconstruction quality but may still struggle to accurately recover fine spatial details across all regions.

A central challenge is preserving local spatial structure while generating consistent optical reflectance. Pixel- or patch-based models can maintain fine details but may produce artifacts across larger areas, whereas global models may smooth out important spatial features. The heterogeneity between SAR and optical signals further complicates the task, requiring architectures capable of modeling complex cross-modal relationships.

In this work, we propose the Bidirectional Mamba Bridged UNet (BiMBU), an image-to-image translation architecture for SAR-to-optical cloud removal. BiMBU combines a U-Net encoder-decoder with a Bidirectional Mamba bottleneck, which processes spatial features in both directions to enhance feature representation while preserving high-resolution spatial details.


