\chapter{Introduction}
Earth observation has become an indispensable tool for understanding and monitoring the planet’s dynamic processes. Optical and radar remote sensing represent two complementary modalities at the core of modern Earth observation systems. Optical sensors provide rich spectral and visual information suitable for human interpretation. This information is used, among others, in forest prediction, agricultural monitoring, military, etc~\cite{S2MS_GAN}. However, they are inherently constrained by atmospheric conditions such illumination variability and in particular cloud cover. This causes considerable data gaps in both the spatial and temporal domains~\cite{CR_SEN2_dRNN}. 
In contrast, synthetic aperture radar (SAR) sensors operate in the microwave domain, offering all-weather, day-and-night imaging capabilities independent of sunlight or cloud interference. However, the backscatter-based nature of SAR imagery introduces challenges related to speckle noise, geometric distortions, and the absence of spectral color information~\cite{bench_sar_color, CR_RS_GAN_s2o}, complicating its interpretation.

To bridge the gap between these two sensing modalities, SAR-to-optical image translation has emerged as a powerful generative approach. It aims to synthesize optical-like, cloud-free imagery from SAR data, combining the interpretability of optical observations with the robustness of radar acquisitions. Recent advances in generative artificial intelligence (GenAI), particularly in conditional generative adversarial networks (cGANs) and diffusion models, have made it possible to learn complex mappings between SAR and optical domains with remarkable realism~\cite{cr_fuse_HR_GEN}. 
These developments have opened new pathways for applications in land-cover classification, vegetation monitoring, disaster response, and particularly, cloud removal. % keep or delete?

Despite the rapid progress in this field, most existing studies have focused primarily on reconstructing the visible RGB subset of optical imagery, with a few extending to the NIR range, leaving the full multispectral potential of missions such as Sentinel-2 largely underexplored. Furthermore, while SAR-to-optical translation is often evaluated qualitatively, systematic assessments of its reliability across individual spectral bands remain limited. Finally, although the approach shows promise for cloud removal, its effectiveness in this context has not yet been comprehensively validated across the full spectral range

Against this background, the present thesis investigates the use of generative models for translating Sentinel-1 SAR imagery into multispectral Sentinel-2 optical data. The work is guided by three main objectives:
\begin{enumerate}
    \item To validate SAR-to-optical image translation across the full 13 Sentinel-2 spectral bands. 
    \item To assess how reliably each optical band can be individually reconstructed and to what extent.
    \item To evaluate the performance of SAR-to-optical image translation for cloud removal
\end{enumerate}

Through these objectives, this thesis aims to provide a comprehensive and quantitative understanding of SAR-to-optical translation as a multimodal learning problem, highlighting its strengths, limitations, and potential for operational cloud-free optical data generation.

\bigskip
The remainder of this thesis is organized as follows. Chapter~\ref{chapter:background} establishes the theoretical foundation by outlining the principles of remote sensing, introducing the Sentinel missions, and reviewing key concepts in cloud removal and generative artificial intelligence, with a focus on SAR-to-optical image translation. Building upon this foundation, Chapter~\ref{chapter:literature} reviews the related literature, highlighting existing methods for SAR–optical data fusion and cloud removal, and identifying the research gaps this work aims to address. Chapter~\ref{chapter:methodology} then presents the methodological framework, describing the dataset preparation, preprocessing steps, and the adopted Pix2Pix model along with its training setup and evaluation procedure. The experimental findings are detailed in Chapter~\ref{chapter:results}, which report the reconstruction results across the full spectral range and assess the model’s capability for cloud removal. To further analyze the model’s behavior, Chapter~\ref{chapter:ablation} conducts ablation studies investigating the influence of different loss configurations and the impact of excluding specific spectral bands. Chapter~\ref{chapter:challenges} follows with a discussion of the main challenges encountered during training, such as instability and artifact formation, and the strategies used to overcome them. Chapter~\ref{chapter:limitations} addresses the limitations of the proposed approach and outlines potential directions for future work, including improved temporal generalization and diffusion-based alternatives. Chapter~\ref{chapter:discussion} integrates and discusses the overall findings in relation to the research objectives, leading to Chapter~\ref{chapter:conclusion}, which concludes the thesis with a summary of key contributions and final remarks.