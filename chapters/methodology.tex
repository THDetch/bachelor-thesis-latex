\chapter{Methodology}
This chapter outlines the methodological framework adopted for the SAR-to-optical translation task. It details the formulation of the problem as a supervised image-to-image regression, describes the datasets and preprocessing procedures used, and presents the model architectures and training strategies employed. In particular, the Pix2Pix conditional generative adversarial network (cGAN) serves as the core model for learning the mapping between Sentinel-1 SAR inputs and Sentinel-2 multispectral optical outputs. The chapter further elaborates on the design of the loss functions, optimization process, and experimental setup used to ensure stable and effective training. Finally, the evaluation metrics are introduced to provide both quantitative and perceptual assessments of the model’s performance, encompassing spatial fidelity, structural consistency, and spectral accuracy.
\section{Problem Formulation}

The task of SAR-to-optical translation can be formulated as a supervised image-to-image regression problem. Let
\[
    X \in \mathbb{R}^{H \times W \times 2}
\]
denote the input SAR patch, where the two channels correspond to Sentinel-1 backscatter in VV and VH polarizations (expressed in the decibel scale(dB)). The corresponding reference optical patch is represented as
\[
    Y \in \mathbb{R}^{H \times W \times N},
\]
where \(N\) denotes the number of channels corresponding to the multispectral bands of {Sentinel-2}.

The objective is to learn a parametric mapping function
\[
    f_\theta : \mathbb{R}^{H \times W \times 2} \; \rightarrow \; \mathbb{R}^{H \times W \times N},
\]
such that the generated multispectral image
\[
    \hat{Y} = f_\theta(X)
\]
is as close as possible to the reference image \( Y \).

Training this model requires minimizing a loss function \(\mathcal{L}\) that measures the discrepancy between \(\hat{Y}\) and \(Y\):
\[
    \hat{\theta} = \arg\min_\theta \; \mathcal{L}(f_\theta(X), Y).
\]

In practice, \(\mathcal{L}\) is designed as a weighted combination of complementary terms (e.g., reconstruction, adversarial, perceptual, and spectral losses) that collectively encourage pixel-level accuracy, structural consistency, and spectral fidelity.

\section{Datasets}
\label{sec:datasets}
\subsection{SEN12-MS}
This thesis relies exclusively on the SEN12MS dataset~\cite{sen12ms_2019}, curated by Schmitt et al.. SEN12MS is a large-scale, globally distributed benchmark explicitly designed to advance research in multimodal Earth observation and deep learning. It comprises 180,662 georeferenced image triplets, each consisting of (i) dual-polarized Sentinel-1 synthetic aperture radar (SAR) data in VV and VH polarization ($\sigma^{0}$ backscatter values in decibel scale), (ii) full Sentinel-2 multispectral imagery spanning all 13 bands, and (iii) MODIS land cover maps derived from the MCD12Q1 product and resampled to 10 m resolution. Each triplet is stored as a 256 × 256 pixel GeoTIFF at 10 m ground sampling distance, corresponding to a spatial coverage of approximately 2.56 × 2.56 km per patch. The dataset has a total size of 510 GB, reflecting its high complexity, diversity, and spatial resolution.

The Sentinel-1 component originates from ground-range-detected (GRD) products acquired in interferometric wide swath (IW) mode. These data were radiometrically calibrated and orthorectified against SRTM or ASTER digital elevation models to ensure accurate geolocation. The Sentinel-2 imagery was curated using a cloud-free mosaicking workflow on Google Earth Engine: within each region of interest (ROI), multiple observations collected during a given meteorological season of 2017 were composited such that cloud-contaminated pixels were systematically excluded. This procedure ensured that every ROI is represented by seasonally consistent, nearly cloud-free multispectral data. Finally, the MODIS land cover maps were used to generate categorical reference layers; however, due to their relatively coarse native resolution (500 m), they are subject to spatial inaccuracies even after upsampling.

Importantly, all triplets underwent manual verification by a remote sensing expert. This revision step ensured that each patch is free from major artifacts, severe registration errors, or residual cloud contamination, thereby guaranteeing the dataset’s quality and usability for machine learning tasks.

The ROIs were sampled globally across all inhabited continents and four meteorological seasons of 2017 to maximize spatial and temporal diversity, as illustrated in Figure~\ref{fig:sen_12_ms_dist}. Nevertheless, it should be noted that the ROI selection was not purely random. In practice, locations were chosen to avoid large homogeneous areas such as deserts or oceans and to ensure inclusion of diverse land cover classes. While this design improves the dataset’s representativeness for a wide range of applications, it may introduce a bias toward heterogeneous landscapes and thus does not fully capture the true global distribution of land cover types.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.95\textwidth]{img/sen_12_ms_distribution.png}
  \caption[ROIs distribution of the SEN12-MS Dataet]{ROIs distribution of the SEN12-MS Dataet. Adapted from \cite{sen12ms_2019}}
  \label{fig:sen_12_ms_dist}
\end{figure}

For the purpose of this thesis, which addresses translation from SAR to multispectral optical imagery, only the Sentinel-1 and Sentinel-2 modalities are employed. Representative sample pairs are illustrated in Figure~\ref{fig:sen12ms_pairs}. The MODIS land cover products included in SEN12MS are disregarded, as they are not directly relevant to the translation task.

\begin{figure}[h!]
    \centering
    % First row: SAR images
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs2017_winter_s1_68_p100.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs1970_fall_s1_105_p100.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs1970_fall_s1_128_p100.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs2017_winter_s1_104_p101.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs1970_fall_s1_145_p100.png}
    \end{subfigure}

    \begin{subfigure}{0.18\textwidth}
        \centering
        {\footnotesize \textit{Winter ROI-68-100}}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \centering
        {\footnotesize \textit{Fall ROI-105-100}}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \centering
        {\footnotesize \textit{Fall ROI-128-100}}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \centering
        {\footnotesize \textit{Winter ROI-104-101}}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \centering
        {\footnotesize \textit{Fall ROI-145-100}}
    \end{subfigure}
    
    \vspace{0.5em}

    % Second row: MS images
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs2017_winter_s2_68_p100.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs1970_fall_s2_105_p100.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs1970_fall_s2_128_p100.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs2017_winter_s2_104_p101.png}
    \end{subfigure}
    \begin{subfigure}{0.18\textwidth}
        \includegraphics[width=\linewidth]{img/ROIs1970_fall_s2_145_p100.png}
    \end{subfigure}

    \caption[Sample SAR–optical pairs from SEN12MS]{Sample pairs from the SEN12MS dataset. Top row: Sentinel-1 SAR patches (R: VV, G: VH, B: VV/VH). Bottom row: corresponding Sentinel-2 multispectral patches (only RGB bands).}
    \label{fig:sen12ms_pairs}
\end{figure}



\subsection{SEN12 datasets Family}
SEN12MS is part of a broader line of datasets developed to foster multimodal remote sensing research. Its direct predecessor, SEN1-2~\cite{sen12_2018}, curated by the same research group, contained approximately 282,000 paired patches of Sentinel-1 VV data and Sentinel-2 RGB composites. While groundbreaking in bridging SAR and optical domains, SEN1-2 lacked georeferencing, full spectral coverage, and multi-polarization SAR, limiting its applicability for remote sensing research beyond proof-of-concept image translation.

SEN12MS addressed these limitations by introducing full multispectral coverage, dual-polarized SAR, geocoded products, and auxiliary land cover labels, making it a comprehensive multimodal benchmark. Building upon this foundation, the dataset family has since been extended. SEN12MS-CR~\cite{sen12ms-cr_2021} added temporally matched cloudy and cloud-free Sentinel-2 imagery alongside Sentinel-1 data, enabling the development and benchmarking of cloud removal methods under realistic atmospheric conditions. Subsequently, SEN12MS-CR-TS~\cite{sen12ms-cr-ts_2022} expanded the concept into the temporal domain, providing year-long multimodal time series with 30 co-registered Sentinel-1 and Sentinel-2 acquisitions per ROI. This evolution reflects a progression from simplified SAR–optical pairs, to globally diverse multimodal data, to temporally rich resources designed for time-series analysis and robust cloud removal.

A comparsion of these different datasets is provided in Table~\ref{tab:sen12_datasets}. In this thesis, however, the focus remains on the SEN12MS dataset, leveraging its multimodal SAR and multispectral imagery for the study of SAR-to-optical translation.

\newcolumntype{P}[1]{>{\raggedright\arraybackslash}p{#1}}

\begin{table}[!h]
    \centering
    \caption{Comparison of SEN12-family datasets.}
    \label{tab:sen12_datasets}
    \setlength{\tabcolsep}{4pt} % tighter horizontal padding
    \renewcommand{\arraystretch}{1.15} % a bit more vertical room
    \begin{adjustbox}{max width=\textwidth, keepaspectratio=false}
    \begin{tabular}{P{2.6cm} P{3.4cm} P{3.4cm} P{3.6cm} P{3.6cm}}
        \toprule
        \textbf{Aspect} &
        \textbf{SEN1-2}~\cite{sen12_2018} &
        \textbf{SEN12MS}~\cite{sen12ms_2019} &
        \textbf{SEN12MS-CR}~\cite{sen12ms-cr_2021} &
        \textbf{SEN12MS-CR-TS}~\cite{sen12ms-cr-ts_2022} \\
        \midrule
        \textbf{Year released} &
        2018 & 2019 & 2021 & 2022 \\
        \addlinespace[6pt]
        \textbf{Main purpose} &
        Proof-of-concept SAR–optical translation &
        Multimodal learning and data fusion &
        Cloud removal with real cloudy/clear pairs &
        Multi-temporal cloud removal (sequence models) \\
        \addlinespace[6pt]
        \textbf{Modalities} &
        S1 (VV), S2 (RGB) &
        S1 (VV,VH), S2 (13 bands), MODIS LULC &
        S1 (VV,VH), S2 (13 bands; cloudy \& cloud-free) &
        S1 (VV,VH), S2 (13 bands; cloudy \& cloud-free time series) \\
        \addlinespace[6pt]
        \textbf{Georeferencing} &
        Not georeferenced &
        Fully georeferenced &
        Fully georeferenced &
        Fully georeferenced \\
        \addlinespace[6pt]
        \textbf{Spatial sampling} &
        Global patch pairs (282k) &
        180,662 patch triplets across 2017 seasons &
        169 ROIs; $>$100k patch triplets &
        53 ROIs; 30 time steps per ROI \\
        \addlinespace[6pt]
        \textbf{Temporal coverage} &
        Single time-point &
        Seasonal (2017) &
        Seasonal with paired cloudy/clear &
        Year-long time series (2018) \\
        \addlinespace[6pt]
        \textbf{Patch size} &
        $256\times256$ px &
        $256\times256$ px &
        $256\times256$ px &
        $256\times256$ px \\
        \addlinespace[6pt]
        \textbf{Notable limitations} &
        RGB only; VV only; no geocoding &
        MODIS labels are coarse (upsampled) &
        Mono-temporal pairs (no full time series) &
        Fewer ROIs; large storage ($\sim$2\,TB) \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
\end{table}

\subsection{Subset Selection \& Preprocessing}
\label{subsec:preprocessing}
The SEN12-MS dataset is divided into four subsets, each corresponding to a meteorological season. Due to the large size of the full dataset, which makes conducting multiple experiments and ablations computationally demanding, only the winter subset (the smallest) was used for training and experiments. This subset contains 31,825 paired images, each consisting of dual-polarized Sentinel-1 SAR data and 13-band Sentinel-2 optical imagery. A custom Python preprocessing pipeline was implemented using \textsc{Rasterio}, \textsc{NumPy}, and \textsc{GDAL} to efficiently stream, normalize, and store the data in a ready-to-train format.  

Following common practices in the literature, Sentinel-1 backscatter values (in decibels) were clipped to fixed physical ranges of \(-25\,\text{dB}\) to \(0\,\text{dB}\) for the VV channel and \(-32.5\,\text{dB}\) to \(0\,\text{dB}\) for the VH channel to suppress noise and radiometric outliers. Similarly, Sentinel-2 reflectance values were clipped to the range \(0\) to \(10{,}000\), corresponding to top-of-atmosphere scaled reflectances. After extensive experimentation with different clipping strategies, including per-band clipping based on the 2nd and 98th percentiles, the fixed ranges commonly used in the literature were found to produce the most consistent and stable results. 

After clipping, all values were linearly normalized to the \textit{tanh} range \([-1, 1]\), consistent with the \texttt{Tanh} activation used in the generator’s output layer. The original spatial resolution of \(256 \times 256\) pixels was maintained, and no data augmentation was applied, as the dataset already exhibits substantial spatial and spectral diversity.

\section{Pix2Pix Model}
% \subsection{Pix2Pix Model}
The image translation task in this thesis is addressed using the Pix2Pix model, introduced by Isola et al.~\cite{pix2pix_2018}. The Pix2Pix framework has gained widespread popularity in the field of SAR-to-optical image translation and is commonly employed as a baseline in nearly all studies addressing this problem. Owing to its proven effectiveness and extensive use in related work, it was selected as the primary model for the experiments conducted in this thesis.

Pix2Pix is based on the concept of conditional generative adversarial networks (cGANs), which extend the original GAN formulation by conditioning both the generator and discriminator on an input image. In this setup, the generator $G$ learns to map an input image $x$ to an output image $y$, while the discriminator $D$ learns to distinguish between real image pairs $\{x, y\}$ and synthesized pairs $\{x, G(x)\}$. This adversarial objective encourages the generated outputs to be both realistic and structurally consistent with the given input.


Formally, the cGAN loss is defined as:
\begin{equation}
    \mathcal{L}_{cGAN}(G,D) = \mathbb{E}_{x,y}[\log D(x,y)] + \mathbb{E}_{x}[\log(1 - D(x,G(x)))].
\end{equation}
To encourage fidelity to the target image, the adversarial loss is combined with an $\ell_{1}$ reconstruction loss:
\begin{equation}
    \mathcal{L}_{\ell_1}(G) = \mathbb{E}_{x,y}[\|y - G(x)\|_1].
\end{equation}
The final objective is then:
\begin{equation}
    G^* = \arg \min_G \max_D \; \mathcal{L}_{cGAN}(G,D) + \lambda \mathcal{L}_{\ell_1}(G),
\end{equation}
where $\lambda$ balances realism and reconstruction accuracy. Following Isola et al., $\lambda = 100$ is typically used. In addition, supplementary loss components were incorporated, as they demonstrated improved training convergence and led to higher-quality reconstructions (see the ablation study on loss functions in Section~\ref{subsec:ablation_loss}).

The generator is implemented as a U-Net encoder–decoder~\cite{U-net_2015}. Unlike a plain encoder–decoder, U-Net introduces skip connections between corresponding downsampling and upsampling layers, allowing low-level spatial details from the input to directly propagate to the output. This design is particularly effective in tasks where the input and output share spatial structures, such as SAR-to-optical translation.

The discriminator follows a PatchGAN architecture, which classifies local $N \times N$ image patches as real or fake instead of operating on the entire image~\cite{pix2pix_2018}. This approach emphasizes high-frequency accuracy and enforces local realism, while the $\ell_1$ term ensures global structural coherence. The original work demonstrated that a patch size of $70 \times 70$ provides a good trade-off between reconstruction quality and computational efficiency.

During training, updates alternate between optimizing $D$ to improve its ability to classify real and generated pairs, and optimizing $G$ to both deceive $D$ and minimize the $\ell_1$ distance to the target image. The Adam optimizer~\cite{adam_optimizer_2017} is used with a learning rate of $1 \times 10^{-4}$ and momentum parameters $\beta_1 = 0.5$ and $\beta_2 = 0.999$. Dropout is applied during both training and inference to introduce stochasticity, although the generated outputs remain largely deterministic in practice.

Overall, the Pix2Pix framework provides a principled and general-purpose solution for image-to-image translation tasks. Its ability to combine adversarial and reconstruction-based objectives makes it particularly suitable for SAR-to-optical translation, where both structural accuracy and perceptual quality are essential.


\section{Training Procedure}
This section describes the complete training pipeline adopted for the proposed SAR-to-optical image translation model. It outlines the experimental setup, loss formulations, optimization strategy, and monitoring procedures that together ensure stable adversarial training and effective convergence. Emphasis is placed on the integration of multiple complementary loss functions to enhance perceptual and structural fidelity, as well as on the staged optimization scheme designed to mitigate common GAN instabilities. The overall workflow aims to achieve a robust balance between pixel-level accuracy, perceptual realism, and generalization performance across diverse imaging conditions.

\subsection{Experimental Setup} All experiments were implemented in Python~3.11, PyTorch 2.4.0, and CUDA 12.4, using the official Pix2Pix implementation\footnote{\url{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}}. Training was conducted on an NVIDIA RTX~A5000 GPU with CUDA acceleration. The training, validation, and test sets were derived from the preprocessed SEN12-MS subset described in Section~\ref{subsec:preprocessing}. Each sample consisted of a Sentinel-1 SAR input patch (VV and VH channels) paired with the corresponding Sentinel-2 optical patch comprising 13 spectral bands.

\subsection{Loss Functions}
\label{subsec:losses}
The loss function employed in the experiments consisted of multiple components. Conditional GAN (cGAN) models, such as Pix2Pix, typically optimize a weighted sum of an adversarial loss and a reconstruction loss. By default, Pix2Pix uses the Binary Cross-Entropy (BCE) loss for the adversarial component and the Mean Absolute Error (MAE), also referred to as the $\mathrm{L1}$ loss, for image reconstruction. However, during preliminary experiments, training instabilities and slow divergence were observed, suggesting the presence of vanishing gradient issues. To mitigate these effects, the adversarial BCE loss was replaced with the Least Squares GAN (LSGAN) loss, implemented as the Mean Squared Error (MSE) loss, which is known to provide more stable gradients and smoother convergence.

In addition to the LSGAN and $\mathrm{L1}$ losses, the LPIPS and SSIM losses were also incorporated during training. Experiments with different combinations of these loss terms indicated that the best results were achieved when all four were combined. 

The overall objective function can be expressed as
\begin{equation}
\mathcal{L}_{\text{total}} =
\mathcal{L}_{\text{GAN}} +
\lambda_{\text{L1}} \mathcal{L}_{\text{L1}} +
\lambda_{\text{SSIM}} \mathcal{L}_{\text{SSIM}} +
\lambda_{\text{LPIPS}} \mathcal{L}_{\text{LPIPS}}.
\end{equation}

where:
\begin{itemize}
    \item $\mathcal{L}_{\text{GAN}}$ denotes the adversarial loss (LSGAN), encouraging the generator to produce outputs indistinguishable from real images. 
    \item $\mathcal{L}_{\text{L1}}$ represents the pixel-wise reconstruction loss (Mean Absolute Error), enforcing global structural similarity between the generated and target images.
    \item $\mathcal{L}_{\text{SSIM}}$ is the Structural Similarity Index Measure loss, promoting local structural consistency.
    \item $\mathcal{L}_{\text{LPIPS}}$ denotes the Learned Perceptual Image Patch Similarity loss, which captures high-level perceptual differences.
    \item $\lambda_{\text{L1}}, \lambda_{\text{SSIM}},$ and $\lambda_{\text{LPIPS}}$ are weighting coefficients that control the relative contribution of each term.
\end{itemize}

In the experiments, the weighting coefficients were set to
$\lambda_{\text{L1}} = 100.0$, 
$\lambda_{\text{SSIM}} = 50.0$, and 
$\lambda_{\text{LPIPS}} = 50.0$.
These values were empirically determined to balance pixel-level accuracy, perceptual quality, and adversarial realism. 

To further assess the individual contribution of each loss component, an ablation study was conducted. The model was trained under different configurations, including combinations of the individual losses and the full combined objective. Quantitative and qualitative results of this ablation are presented and discussed in Section~\ref{subsec:ablation_loss}. Except for the experiments dedicated to this loss ablation, all other trainings were performed using the full combination of the four loss functions.

\subsection{Training Strategy}
The network training followed a staged adversarial optimization scheme with alternating updates of the generator and discriminator. At the early stage of training, the generator was warmed up by being trained independently for the first 20~epochs to stabilize its predictions before introducing the discriminator. This step mitigates early training instabilities commonly observed in GAN-based frameworks, which were also encountered during experimentation. After the warm-up period, both networks were jointly optimized in an alternating fashion, where the discriminator was updated once per iteration, followed by a generator update (1:1 ratio between $G$ and $D$). The discriminator was trained to minimize the least-squares error between its predictions and the target real/fake labels, while the generator was optimized to minimize the composite objective described in the previous subsection.

Training was performed for 150~epochs using a batch size of 16 and the Adam optimizer~\cite{adam_optimizer_2017} with a learning rate of $1\times10^{-4}$ and momentum parameters $(\beta_1, \beta_2) = (0.5, 0.999)$. To prevent overfitting and accelerate convergence, the generator’s learning rate was adaptively reduced using the \texttt{ReduceLROnPlateau} scheduler, which halves the rate when the validation $\mathrm{L1}$ loss stagnates. The discriminator’s learning rate was kept constant throughout training to maintain stable adversarial dynamics, as reducing it was found to introduce additional instability during preliminary experiments.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth,height=0.65\textheight]{img/training_curves.png}
    \caption{Training and validation loss curves over 150~epochs.}
    \label{fig:training_curve}
\end{figure}

Figure~\ref{fig:training_curve} illustrates the evolution of the training and validation losses over 150~epochs, together with the learning rate behavior. The training loss exhibits a steady decline, indicating stable convergence of the generator–discriminator optimization. The validation loss follows a similar trend with minor fluctuations, reflecting the stochastic nature of adversarial learning. The learning rate decreases automatically when the validation $\mathrm{L1}$ loss stagnates, as controlled by the \texttt{ReduceLROnPlateau} scheduler, leading to smoother convergence during the later stages of training.

\subsection{Monitoring and Checkpointing}
Model performance was monitored using both quantitative validation metrics and qualitative visualizations. After each epoch, the validation set was evaluated to compute the $\mathrm{L1}$, SSIM, and LPIPS metrics. The model achieving the lowest validation $\mathrm{L1}$ loss was saved as the best-performing checkpoint. Additionally, intermediate qualitative samples were generated every ten epochs by translating a fixed subset of Sentinel-1 patches to their corresponding optical outputs, allowing visual inspection of the training progression.

All relevant training statistics, including generator and discriminator losses, learning rates, and validation metrics, were logged for subsequent analysis. Checkpoints were saved at the end of each epoch to enable resumption in case of interruptions. After training completion, the recorded metrics were used to generate loss and performance curves, which facilitated evaluation of convergence stability and model generalization.


\section{Evaluation Metrics}
The effectiveness of SAR-to-optical image translation depends not only on the choice of translation models but also on the methods employed for quality assessment. Image Quality Assessment (IQA) serves two key purposes: (i) to objectively evaluate the quality of results produced by different models, and (ii) to guide the optimization of network architectures and algorithms~\cite{quality_assessment_S2OT}.

In~\cite{quality_assessment_S2OT}, five image quality assessment (IQA) metrics—SSIM, FSIM, MSE, LPIPS, and DISTS—were systematically evaluated through image restoration experiments to determine their suitability for SAR-to-optical translation tasks. The study found that SSIM, MSE, and LPIPS exhibited strong consistency with human visual perception, demonstrated stable convergence, and effectively captured both structural and textural characteristics. In contrast, FSIM often failed to represent fine details accurately, while DISTS showed instability. Consequently, the authors recommended SSIM, MSE, and LPIPS as complementary metrics for assessing pixel-level accuracy, structural similarity, and perceptual quality, respectively. However, based on the analysis conducted in this thesis across 16 related studies, SSIM, PSNR, and SAM remain the most widely adopted evaluation metrics in SAR-to-optical translation, fusion, and cloud removal research, whereas LPIPS and MSE appear less frequently, as summarized in Table~\ref{tab:iqa}. This trend aligns with the observations reported in the literature survey by~\cite{sar_2_opt_CGAN_survey_taxonomy}.

\begin{table}[h!]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Metric} & \textbf{References} & \textbf{Frequency} \\
\midrule
Structural Similarity Index Measurement (SSIM)~\cite{iqa_ssim}
 & \cite{CR_Advances_Review_ORS, RS_Data_Fusion_GANs_sota, DiffCR, c_diffusion_s2o, s2o_ViT_cGAN, S2MS_GAN, c_guided_fus_s2ot, transfusion_cr, trans_gan_CF, hvt_cgan, msf_gan, diffusion_memory} 
 & 12 \\
Peak Signal-to-Noise Ratio (PSNR)~\cite{iqa_psnr}
 & \cite{CR_Advances_Review_ORS, DiffCR, CR_RS_spati_atten_GAN, s2o_ViT_cGAN, CR_RS_GAN_s2o, S2MS_GAN, c_guided_fus_s2ot, transfusion_cr, trans_gan_CF, hvt_cgan, msf_gan, diffusion_memory} 
 & 12 \\
Spectral Angle Mapper (SAM)~\cite{iqa_sam}
 & \cite{aCGAN_fuse_sar_MS, RS_Data_Fusion_GANs_sota, CR_RS_GAN_s2o, S2MS_GAN, c_guided_fus_s2ot, transfusion_cr, trans_gan_CF, cond_brownian, hvt_cgan, msf_gan} 
 & 11 \\
Fréchet Inception Distance (FID)~\cite{iqa_fid}
 & \cite{DiffCR, c_diffusion_s2o, s2o_ViT_cGAN, cond_brownian, hvt_cgan, msf_gan} 
 & 6 \\
Root Mean Square Error (RMSE) 
 & \cite{aCGAN_fuse_sar_MS, CR_Advances_Review_ORS, RS_Data_Fusion_GANs_sota, CR_RS_GAN_s2o, c_guided_fus_s2ot} 
 & 5 \\
Learned Perceptual Image Patch Similarity (LPIPS)~\cite{iqa_lpips}
 & \cite{CR_Advances_Review_ORS, DiffCR, S2MS_GAN, cond_brownian, diffusion_memory} 
 & 5 \\
Mean Absolute Error (MAE) 
 & \cite{CR_RS_GAN_s2o, c_guided_fus_s2ot} 
 & 2 \\
Mean Square Error (MSE) 
 & \cite{CR_RS_spati_atten_GAN, trans_gan_CF} 
 & 2 \\
\bottomrule
\end{tabular}
\caption[Common evaluation metrics in SAR-to-optical and cloud removal]{Frequency of common evaluation metrics used in SAR-to-optical and cloud-removal studies.}
\label{tab:iqa}
\end{table}

\paragraph{SSIM}
The Structural Similarity Index (SSIM)~\cite{iqa_ssim} measures perceptual similarity by comparing local patterns of luminance, contrast, and structure between two images. Unlike pixel-wise errors, it models human visual sensitivity to structural distortions~\cite{DiffCR,hvt_cgan}, which is crucial for evaluating translated images. For two images $x$ and $y$, SSIM is defined as
\begin{equation}
\text{SSIM}(x,y) = \frac{(2\mu_x \mu_y + c_1)(2\sigma_{xy} + c_2)}{(\mu_x^2 + \mu_y^2 + c_1)(\sigma_x^2 + \sigma_y^2 + c_2)},
\end{equation}
where $\mu_x, \mu_y$ are means, $\sigma_x^2, \sigma_y^2$ variances, and $\sigma_{xy}$ the covariance. Values close to 1 indicate strong structural similarity. By focusing on local patterns of pixel intensities and their structural relationships, SSIM better reflects perceptual fidelity compared to raw pixel-difference metric


\paragraph{PSNR} 
The Peak Signal-to-Noise Ratio (PSNR) quantifies the distortion between a reconstructed image and its reference. PSNR is directly related to the Mean Squared Error (MSE), measuring pixel-level fidelity by comparing the residual error to the maximum possible signal intensity. For two images $x$ and $y$, PSNR is defined as
\begin{equation}
\text{PSNR}(x,y) = 10 \cdot \log_{10} \left( \frac{MAX^2}{\text{MSE}(x,y)} \right),
\end{equation}
with
\begin{equation}
\text{MSE}(x,y) = \frac{1}{N} \sum_{i=1}^{N} (x_i - y_i)^2,
\end{equation}
where $x_i$ and $y_i$ denote the pixel values of the generated and reference images, $N$ is the total number of pixels, and $MAX$ is the maximum pixel intensity (typically $255$ for 8-bit images).  

Higher PSNR values indicate lower distortion and better image quality, as they imply that the reconstructed image more closely approximates the reference. Despite its popularity for tasks such as denoising and compression, PSNR is limited by its purely pixel-wise formulation and often correlates weakly with human visual perception~\cite{DiffCR}.

\paragraph{SAM} 
The Spectral Angle Mapper (SAM), originally proposed by Kruse et al.~\cite{iqa_sam} in 1993, is widely employed in remote sensing to evaluate the spectral fidelity of reconstructed images. SAM regards the spectrum of each pixel as a high-dimensional vector and quantifies similarity by measuring the angle between the generated and reference spectral vectors. For two spectral vectors $x$ and $y$, SAM is defined as
\begin{equation}
\text{SAM}(x,y) = \arccos \left( \frac{\langle x, y \rangle}{\|x\|_2 \cdot \|y\|_2} \right),
\end{equation}
where $\langle x,y \rangle$ denotes the dot product and $\|\cdot\|_2$ is the Euclidean norm.  

SAM is typically expressed in degrees, with smaller values indicating higher spectral similarity and less distortion. Since it only considers the direction of the spectral vectors and not their magnitude, SAM is invariant to changes in illumination, making it particularly suitable for remote sensing and multispectral image analysis~\cite{S2MS_GAN}. In practice, the global SAM score is computed as the average angle across all pixels in the image.

\paragraph{LPIPS} 
The Learned Perceptual Image Patch Similarity (LPIPS) metric was proposed by Zhang et al.~\cite{iqa_lpips} to provide a perceptual measure of image similarity that better aligns with human visual judgment. It compares feature activations from pretrained convolutional networks, thereby capturing high-level semantics and perceptual realism. For two images $x$ and $y$, LPIPS is defined as
\begin{equation}
\text{LPIPS}(x,y) = \sum_{l} w_l \cdot \| f_l(x) - f_l(y) \|_2,
\end{equation}
where $f_l(\cdot)$ denotes the feature representation in the $l$-th layer of the network and $w_l$ is a learned weight.  

By measuring differences in a deep feature space rather than raw pixel intensities, LPIPS reflects perceptual similarity and visual realism. Lower LPIPS values indicate that the generated image is closer to the reference in terms of human-perceived quality~\cite{CR_Advances_Review_ORS,DiffCR}, making this metric particularly useful for evaluating the naturalness of translated images.

\paragraph{MAE \& RMSE}
Similar to PSNR, the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) evaluate image reconstruction quality on a pixel-wise level. They are defined as follows:
\begin{equation}
\mathrm{MAE} = \frac{1}{N} \sum_{i=1}^{N} \left| y_i - x_i \right|
\end{equation}

\begin{equation}
\mathrm{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \left( y_i - x_i \right)^2}
\end{equation}

where $x_i$ and $y_i$ denote the pixel intensity values of the reference (ground-truth) and generated images, respectively, and $N$ represents the total number of pixels, defined as $N = C \times H \times W$, with $C$, $H$, and $W$ being the number of channels, height, and width of the image.

The Mean Absolute Error (MAE) measures the average absolute difference between corresponding pixels, providing a direct estimate of overall reconstruction accuracy. The Root Mean Square Error (RMSE), in contrast, penalizes larger deviations more heavily due to the squared term, making it more sensitive to outliers and localized reconstruction errors.


% \paragraph{FID} 
% Introduced in 2018 by Heusel at el.~\cite{iqa_fid}, the Fréchet Inception Distance (FID) is a perceptual metric that evaluates the realism of generated images at the distributional level. Instead of comparing images pixel by pixel, FID measures the distance between the feature distributions of generated and reference images, extracted by a pretrained Inception network. Let $(\mu_r, \Sigma_r)$ and $(\mu_g, \Sigma_g)$ denote the mean and covariance of the reference and generated feature distributions, respectively. FID is defined as
% \begin{equation}
% \text{FID} = \| \mu_r - \mu_g \|_2^2 + \text{Tr}\left( \Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2} \right).
% \end{equation}

% Lower FID values indicate closer alignment between generated and real image distributions. While LPIPS assesses pairwise perceptual similarity, FID captures distributional alignment, making the two metrics complementary.

\renewcommand{\arraystretch}{1.25} % increases row height by 25%
\begin{table}[h!]
	\centering
	\caption[Evaluation metrics used summary]{Summary of evaluation metrics for SAR-to-multispectral translation.}
	\begin{tabularx}{\textwidth}{p{1.7cm}X X X}
		\toprule
		\textbf{Metric} & \textbf{Aspect Evaluated} & \textbf{Advantages} & \textbf{Limitations} \\
		\midrule
		PSNR  & Pixel-level fidelity via mean squared error ratio & Simple, widely used, interpretable in terms of noise or distortion & Correlates weakly with human perception; sensitive to pixel shifts \\
		SSIM  & Structural similarity (luminance, contrast, texture) & Captures perceptual structure better than PSNR; patch-based & Still intensity-based; limited correlation with perceptual realism \\
		SAM   & Spectral fidelity across bands & Invariant to illumination; critical for multispectral data integrity & Ignores spatial and structural context; only reflects spectral angle \\
		MAE \& RMSE & Pixel-wise reconstruction accuracy & Provide direct quantitative estimates of average and large-magnitude errors; easy to interpret & Do not account for perceptual or structural quality; sensitive to scaling and outliers (especially RMSE) \\
		LPIPS & Perceptual similarity using deep feature representations & Aligns well with human judgment; sensitive to high-level semantic differences & Requires pretrained CNN; limited to 3-channel inputs unless adapted \\
		\bottomrule
	\end{tabularx}
\end{table}
\renewcommand{\arraystretch}{1.0} % reset to default



\paragraph{Evaluation Protocol} 
The evaluation of the SAR-to-optical translation performance was carried out using a combination of quantitative and perceptual metrics. The primary quantitative metrics included PSNR, SSIM, and SAM, complemented by the perceptual metric LPIPS. PSNR quantifies pixel-level fidelity, SSIM assesses local structural similarity, and SAM measures spectral consistency across all spectral bands, which is particularly important in multispectral remote sensing applications. In addition, conventional error-based metrics, MAE and RMSE, were employed for validation. To further capture perceptual realism beyond pixel-wise statistics, the deep feature–based LPIPS metric was used to enable pairwise comparisons between generated and reference images. For outputs containing more than three spectral bands, the perceptual metric LPIPS were computed on a fixed RGB composite for both the reference and the generated images, and this limitation was explicitly acknowledged. Overall, this combination of metrics provides a comprehensive assessment encompassing spatial fidelity, structural integrity, spectral accuracy, and perceptual quality.
