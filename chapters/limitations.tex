\chapter{Challenges}
\label{chapter:challenges}
\section{Inherent Model Characteristics}
Although the Pix2Pix model demonstrated stable training behavior and produced high-quality image reconstructions, several challenges arose during the training process. These challenges stem from the inherent characteristics of GAN-based architectures, of which Pix2Pix is a representative example. One of the main issues encountered was the vanishing or exploding gradient problem, where the early layers of the network receive minimal updates during backpropagation, leading to slow convergence or even training stagnation. The literature attributes this behavior primarily to the choice of activation functions and optimization strategies. For instance, \cite{CR_RS_GAN_s2o} and \cite{hvt_cgan} address this issue by incorporating residual blocks to improve gradient flow. In the case of Pix2Pix, however, the vanishing gradient problem was mitigated by replacing the default binary cross-entropy (BCE) loss with the least-squares loss function. Unlike BCE, which tends to saturate when the discriminator becomes overconfident, the least-squares formulation penalizes outputs based on their squared distance from the target labels, thereby maintaining non-zero gradients even for well-classified samples~\cite{s2o_ViT_cGAN}.

Moreover, the Pix2Pix model inherently incorporates an $\mathrm{L1}$ loss term. However, a well-known limitation of the $\mathrm{L1}$ loss is that it is not well suited for generating high-resolution or perceptually rich images~\cite{s2o_ViT_cGAN}. To overcome this limitation, additional loss components based on the SSIM and the perceptual LPIPS metric were integrated into the objective function. This enhancement enabled the model to more reliably reproduce both the visual and spectral characteristics of the ground-truth multispectral images.

Another issue encountered during training was the emergence of \textit{checkerboard artifacts} in the generated images. These artifacts appeared as grid-like patterns, particularly visible in homogeneous regions such as water bodies and vegetated surfaces. The phenomenon originates from the use of transposed convolutions in the generator’s upsampling layers, where uneven overlap between convolutional kernels causes certain pixels to receive disproportionately large updates~\cite{checkerboard_deconvolution}. The same issue was also acknowledged in the official Pix2Pix implementation. To mitigate this problem, the transposed convolution layers were replaced with a combination of nearest-neighbor upsampling followed by standard convolution operations (Listing~\ref{lst:resize_conv}). This modification ensured uniform pixel coverage, effectively eliminating checkerboard artifacts and resulting in smoother and more visually coherent image reconstructions.


\begin{lstlisting}[caption={Original transposed convolution block in Pix2Pix}, label={lst:original_convtranspose}]
# Original implementation using ConvTranspose2d
nn.ConvTranspose2d(
    in_channels = ngf * mult,
    out_channels = int(ngf * mult / 2),
    kernel_size = 4,
    stride = 2,
    padding = 1,
    bias = use_bias
)
\end{lstlisting}

\begin{lstlisting}[caption={Modified resize-conv block to mitigate checkerboard artifacts}, label={lst:resize_conv}]
# Replaced with nearest-neighbor upsampling followed by regular convolution
nn.Upsample(scale_factor = 2, mode = 'bilinear'),
nn.ReflectionPad2d(1),
nn.Conv2d(
    in_channels = ngf * mult,
    out_channels = int(ngf * mult / 2),
    kernel_size = 3,
    stride = 1,
    padding = 0
)
\end{lstlisting}


\chapter{Limitaions \& Future Work}
\label{chapter:limitations}
\section{Model-Specific Limitations of GAN-Based Translation}
Despite the remarkable performance achieved by the proposed model, several limitations remain. Since the translation relies solely on SAR data, which inherently contains speckle noise, the trained model struggles to generate realistic optical images when the SAR inputs lack distinct structural information. In such cases, the model appears unable to discern meaningful spatial patterns and instead interprets some parts of the scene as noise, resulting in noise-like optical outputs, as illustrated in Figure~\ref{fig:limitation_noise}.

\begin{figure}[h!]
    \centering
    \setlength{\tabcolsep}{2pt} % horizontal padding between columns (same as ablation)
    \renewcommand{\arraystretch}{1.0} % vertical padding (same as ablation)

    \begin{tabular}{c *{3}{c}}
        % ------------------- Row 1 -------------------
        
        \includegraphics[width=0.2\textwidth, height=0.2\textheight, keepaspectratio]{img/limitation_noise/sample_000071_sar_pseudo.png} &
        \includegraphics[width=0.2\textwidth, height=0.2\textheight, keepaspectratio]{img/limitation_noise/sample_000071_pred_rgb.png} &
        \includegraphics[width=0.2\textwidth, height=0.2\textheight, keepaspectratio]{img/limitation_noise/sample_000071_true_rgb.png} \\
        % ------------------- Row 2 -------------------
        
        \includegraphics[width=0.2\textwidth, height=0.2\textheight, keepaspectratio]{img/limitation_noise/sample_000834_sar_pseudo.png} &
        \includegraphics[width=0.2\textwidth, height=0.2\textheight, keepaspectratio]{img/limitation_noise/sample_000834_pred_rgb.png} &
        \includegraphics[width=0.2\textwidth, height=0.2\textheight, keepaspectratio]{img/limitation_noise/sample_000834_true_rgb.png} \\
    \end{tabular}

    \caption[Model limitation on structureless SAR inputs]{%
    Qualitative examples illustrating the limitation of the SAR-to-optical translation model when the SAR input lacks clear structural information. 
    Columns: 
    \textbf{(i)}~SAR input (pseudo-RGB), 
    \textbf{(ii)}~model-generated optical image, and 
    \textbf{(iii)}~reference cloud-free Sentinel-2 image. 
    }
    \label{fig:limitation_noise}
\end{figure}

This limitation suggests that future research should focus on exploring alternative generative architectures, such as Diffusion Models. Unlike GAN-based approaches, Diffusion Models learn the underlying data distribution by iteratively adding and removing noise, which often results in more stable training and higher-quality image synthesis. Furthermore, while GAN-based models are known for their instability and limited capacity to further enhance image fidelity, Diffusion Models have recently demonstrated superior performance in producing high-resolution, photorealistic outputs. Notably, the current state-of-the-art method for cloud removal on the SEN12-MS-CR dataset (see Section~\ref{sec:datasets}) is a Diffusion-based approach, namely \textit{DiffCR}~\cite{DiffCR}, underscoring the growing effectiveness of these models in handling complex remote sensing translation tasks.

Incorporating attention mechanisms into model architectures has been shown to significantly enhance cloud removal and translation performance. Pan~\cite{CR_RS_spati_atten_GAN} introduced the Spatial Attention GAN (SpA-GAN), which integrates a spatial attention mechanism into the generator to imitate the human visual system by focusing selectively on cloud-contaminated regions. This mechanism enables the model to adaptively allocate attention to cloudy areas, improving the recovery of fine spatial details while preserving unaffected regions. Furthermore, the attention maps are also used in the loss function, helping the model prioritize relevant areas during training. Experimental results on the RICE~\cite{RICE_dataset} dataset demonstrated substantial gains in PSNR and SSIM over conventional cGAN and CycleGAN models. Therefore, incorporating attention modules in future SAR-to-optical translation architectures could improve spatial awareness, enhance feature consistency, and yield more visually and spectrally accurate reconstructions 

\section{Temporal Generalizability Across Different Seasons}
Another important limitation lies in the model’s temporal generalizability. The Pix2Pix model was trained and evaluated primarily on the winter subset of the SEN12-MS dataset, which ensures a consistent data distribution and spectral domain during training. While the model exhibits reasonable spatial generalization when evaluated on the SEN12-MS-CR dataset—which features a distinct set of regions of interest (ROIs) compared to SEN12-MS—it struggles to maintain the same level of performance across different seasonal subsets. In particular, when applied to the summer, fall, or spring subsets, the model demonstrates a noticeable degradation in reconstruction quality, indicating sensitivity to seasonal variability in vegetation, soil moisture, and illumination conditions. Quantitative and qualitative results for the different seasons are provided in Appendix~\ref{appendix:results_seasons}.

These findings indicate that while the model performs reliably under spatial conditions similar to its training distribution, it fails to maintain consistent performance across acquisition periods with differing environmental characteristics. Future research should therefore focus on enhancing both spatial and temporal robustness. Potential strategies include domain adaptation techniques, fine-tuning with representative samples from multiple seasons and regions, and data augmentation methods that simulate seasonal and spatial variability.


\section{Aware per-Band Clipping}

As described in Section~\ref{subsec:preprocessing}, the optical data values were clipped to the range [0, 10,000], regardless of the spectral band. This clipping strategy is consistently adopted across the literature and has therefore been followed in this thesis. However, despite the uniform clipping, the actual value distributions differ significantly among the bands, as revealed by a statistical analysis performed on the winter subset.

From each ROI in the winter subset (47 ROIs), one optical \texttt{.tif} file was analyzed to examine the pixel value distributions across the 13 Sentinel-2 bands. The resulting histograms (Figure~\ref{fig:data_ranges}) show that the dynamic range and distribution shape vary considerably between bands. A particularly distinct observation is seen in the Cirrus band (B10, indexed as 11 in the figure), where the pixel values are compressed into a very limited, discrete set of intensity levels, unlike the continuous distributions of other bands.


\begin{figure}[h!]
    \centering
    % -------- First row --------
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/data_ranges/sample_1.png}
        \label{fig:sub1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/data_ranges/sample_2.png}
        \label{fig:sub2}
    \end{subfigure}

    % -------- Second row --------
    \vspace{0.5em}
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/data_ranges/sample_3.png}
        \label{fig:sub3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\linewidth]{img/data_ranges/sample_4.png}
        \label{fig:sub4}
    \end{subfigure}

    \caption{Value ranges across the individual optical bands. Band numbers correspond to their indices, e.g., B9 in the figure represents B8A and B10 represents B9, etc.}

    \label{fig:data_ranges}
\end{figure}

The statistics are moreover numerically reported in Table~\ref{tab:s2_band_stats}. This uniform clipping of all optical bands to a fixed range such as [0, 10000] disregards the intrinsic spectral variability between bands. Low-range bands become underrepresented, and high-range bands risk saturation, leading to an imbalanced input distribution. This can hinder convergence and degrade the realism of generated optical images. Band-specific normalization based on empirical percentiles provides a more balanced and physically consistent preprocessing strategy. Although these statistics were computed from only 47 optical samples, the analysis provides valuable insights that can inform and guide future research.

\begin{table}[H]
\centering
\caption{Summary statistics of the Sentinel-2 optical bands across the winter subset. Reported are the global minimum and maximum values, and the 1\textsuperscript{st}, 50\textsuperscript{th} (median), and 99\textsuperscript{th} percentiles.}
\label{tab:s2_band_stats}
\begin{tabular}{lccccc}
\toprule
\textbf{Band} & \textbf{Global Min} & \textbf{Global Max} & \textbf{p1} & \textbf{p50} & \textbf{p99} \\
\midrule
B1  (Coastal Aerosol)  & 940  & 3712 & 965  & 1387 & 2034 \\
B2  (Blue)             & 634  & 5759 & 725  & 1101 & 1998 \\
B3  (Green)            & 395  & 6001 & 498  & 960  & 2423 \\
B4  (Red)              & 223  & 6748 & 331  & 893  & 3543 \\
B5  (Red Edge)       & 204  & 6377 & 291  & 1119 & 3758 \\
B6  (Red Edge)       & 139  & 6532 & 253  & 1643 & 4042 \\
B7  (Red Edge)       & 117  & 6875 & 239  & 1834 & 4389 \\
B8  (NIR)              & 185  & 7048 & 202  & 1805 & 4211 \\
B8A (Narrow NIR)       & 108  & 7130 & 199  & 1995 & 4542 \\
B9  (Water Vapour)     & 43   & 3329 & 85   & 720  & 2214 \\
B10 (Cirrus)           & 3    & 303  & 7    & 16   & 192  \\
B11 (SWIR)           & 34   & 6173 & 75   & 1802 & 5777 \\
B12 (SWIR)           & 1    & 5252 & 46   & 1180 & 4825 \\
\bottomrule
\end{tabular}
\end{table}

% \textcolor{red}{consider reporting on mean and standard deviation}