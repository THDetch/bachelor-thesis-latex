\chapter{Conclusion}
This thesis investigated the potential of SAR-to-optical image translation as a generative framework for synthesizing multispectral optical imagery from radar data, with the broader goal of mitigating cloud-related limitations in optical remote sensing. Using co-registered Sentinel-1 and Sentinel-2 data from the SEN12-MS dataset, the Pix2Pix conditional generative adversarial network (cGAN) was trained to perform end-to-end translation from dual-polarized SAR inputs to full-spectrum optical outputs across all 13 Sentinel-2 bands.

The first objective—to validate SAR-to-optical translation across all 13 Sentinel-2 bands—was successfully achieved. The model demonstrated a strong capability to reconstruct high-fidelity optical imagery, confirming that meaningful spectral and spatial correlations exist between the SAR and optical domains.
The second objective—assessing the reliability of individual band reconstruction—showed that reconstruction quality varies with spectral wavelength and intrinsic signal properties. The model achieved its best performance on the 60 m atmospheric correction bands (B1, B9, B10), which exhibit low spatial variability, and its lowest on the near-infrared (B8) band, where SAR sensitivity is limited. This across-band reconstruction behavior may, however, be influenced by the exclusive use of winter data, where NIR information is typically underrepresented.
The third objective—evaluating cloud removal performance—demonstrated that Pix2Pix, and by extension GAN-based models, can effectively generate cloud-free optical imagery solely from SAR inputs. Notably, despite not being explicitly trained for cloud removal nor fine-tuned on the SEN12-MS-CR dataset, the model achieved results that matched or surpassed several state-of-the-art methods. Although Pix2Pix is often reported in the literature as a relatively weak baseline, these findings indicate that with proper hyperparameter optimization, it can reach competitive performance.

Furthermore, ablation studies confirmed that integrating SSIM and LPIPS losses with the standard GAN objective yielded the most consistent results. A second ablation, excluding the 60 m bands, demonstrated both quantitatively and qualitatively that training on the full spectral range improves spectral and structural fidelity.
While the findings affirm the feasibility and practical value of SAR-to-optical translation, several limitations remain. Model performance degrades in spectrally complex or textureless regions, and its generalization across seasons is constrained by training exclusively on winter data due to the computational demands of the full SEN12-MS dataset ($\approx 510$ GB). Moreover, the observed differences in per-band data distributions suggest that uniform per-band clipping may introduce distributional biases that limit spectral realism. Future work should address these issues by employing diffusion-based generative models, incorporating seasonally diverse training data, and adopting per-band-aware preprocessing strategies.

In conclusion, this thesis demonstrates that generative models, particularly GAN-based architectures, can efficiently reconstruct full-spectrum optical imagery from SAR data while achieving competitive cloud removal performance. Despite inherent limitations related to model architecture and preprocessing, the results underscore the promise of generative approaches for enhancing optical data availability under adverse observation conditions and provide a foundation for future improvements.