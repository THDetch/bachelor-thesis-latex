\chapter*{Abstract}
Cloud cover remains a persistent challenge in optical remote sensing, limiting the usability of optical satellite imagery for continuous Earth observation. Synthetic Aperture Radar (SAR) data, in contrast, provides cloud-penetrating, all-weather imaging but lacks the spectral and visual richness of optical observations. Bridging these complementary modalities, this thesis investigates SAR-to-optical image translation — a generative approach that synthesizes optical-like imagery from SAR inputs — using the Pix2Pix conditional generative adversarial network (cGAN).
The study employs the winter subset of the SEN12-MS dataset, which offers globally distributed, co-registered Sentinel-1 and Sentinel-2 imagery. The objectives are threefold: (i) to validate SAR-to-optical translation across all 13 Sentinel-2 spectral bands, (ii) to assess the reliability and reconstructability of each individual band, and (iii) to evaluate the performance of the translation model for cloud removal.
Experimental results show that the model effectively learns the SAR-to-optical mapping and achieves high reconstruction quality across all spectral bands. Bandwise analysis reveals that reconstruction accuracy varies with spectral characteristics. When applied to the SEN12-MS-CR dataset, the model successfully generates cloud-free optical imagery that closely matches reference data, outperforming previous methods and the state-of-the-art DiffCR model.
Overall, the findings confirm the viability of SAR-to-optical translation for producing spectrally consistent, cloud-free optical imagery, thus enhancing the temporal continuity of Earth observation data. Two ablation studies further analyze the impact of different loss functions and the exclusion of 60 m bands. Additionally, based on data analysis, a new per-band clipping strategy for optical data is proposed.