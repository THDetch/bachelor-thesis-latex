\chapter{Literature Review}
\label{chapter:literature}
Cloud contamination in optical remote sensing imagery hinders continuous Earth observation, limiting applications such as crop monitoring and land cover classification. Synthetic Aperture Radar (SAR) systems can penetrate clouds, enabling data acquisition in all weather conditions. This capability makes SAR data valuable for filling gaps in optical time series, driving research into SAR–optical fusion and SAR-to-optical image translation for cloud removal.
Early methods leveraged traditional signal processing techniques. Huang et al.~\cite{huang2015} introduced sparse representation-based cloud removal using SAR data, which Xu et al.~\cite{CR_spars_repre_MT_dict_L} extended via multi-temporal dictionary learning. These approaches, however, struggled under heavy cloud cover or highly dynamic surface changes. 

Deep learning has profoundly transformed the field of remote sensing. The foundational Generative Adversarial Network (GAN) framework by Goodfellow et al.~\cite{GANs_Goodfellow}, later extended to Conditional GANs (cGANs) by Mirza and Osindero~\cite{cGANs_mirza}, enabled image-to-image translation tasks such as SAR-to-optical image synthesis. Soon after their introduction, researchers began employing cGANs to address cloud contamination in optical imagery. 
Enomoto et al.~\cite{filmy_cloudy_NIR_Enomoto} applied a cGAN for cloud removal by fusing the RGB composite of a cloudy optical image with the near-infrared (NIR) band to reconstruct a cloud-free RGB image. However, the approach was limited under dense cloud conditions, as the NIR band cannot fully penetrate thick clouds. Despite this, their work was pivotal in demonstrating the potential of multimodal data fusion for cloud removal, paving the way for subsequent studies.
Building upon this concept, Grohnfeldt et al.~\cite{A_cGAN_fuse_sar_MS_CR} introduced SAR-Opt-cGAN, a model designed to fuse Sentinel-1 SAR and Sentinel-2 optical data—marking the first use of SAR data within a cGAN framework for this purpose. Their adaptation of the Pix2Pix architecture allowed flexible multi-channel inputs and was trained on a subset of the SEN1-2 dataset~\cite{sen12_2018}. The results confirmed the advantage of incorporating SAR data, validating the effectiveness of the approach for mitigating cloud cover.

The same research group from the Technical University of Muich (TUM)~\footnote{https://www.tum.de/} and the German Aerospance Center (DLR)~\footnote{https://www.dlr.de/en} significantly advanced this field by developing a family of co-registered SAR–optical datasets. The initial SEN1-2 dataset~\cite{sen12_2018} combined single-polarized (VV) SAR with RGB optical composites, later extended to dual-polarized SAR and full 13-band optical imagery in SEN12-MS~\cite{sen12ms_2019}. To address cloud contamination, they released SEN12-MS-CR~\cite{sen12ms-cr_2021}, containing triplets of dual-polarized SAR, cloudy optical, and corresponding cloud-free images. Finally, the SEN12-MS-CR-TS dataset~\cite{sen12ms-cr-ts_2022} introduced multi-temporal observations, further enhancing its utility for cloud removal research.

Fuentes Reyes et al.~\cite{sar2opt_cGAN_Optim_oppr_limits} optimized an unsupervised CycleGAN for SAR-to-optical translation, improving interpretability and reducing speckle through customized preprocessing and architectural refinements. Although effective in rural areas, the method struggled to preserve fine structural details in dense urban environments. In contrast, Wang et al.~\cite{s2ot_s_cycle_adv} proposed a Supervised CycleGAN (S-CycleGAN) by introducing a pixel-wise MSE loss, enabling paired SAR–optical training. The model produced realistic optical images and was successfully applied to cloud removal by selectively replacing only cloud-covered regions with corresponding SAR-translated patches.
Gao et al.~\cite{cr_fuse_HR_GEN} advanced the idea through a fusion-based GAN framework for high-resolution imagery. Instead of directly translating SAR to optical, they first generated a simulated optical image from SAR data and then fused it with both SAR and cloud-contaminated optical inputs to reconstruct cloud-free results. An ablation study confirmed that this two-stage fusion strategy achieved the best overall performance among tested configurations.

Recent models have shown increasing architectural complexity. Naderi Darbaghshahi et al.~\cite{naderi2021} introduced a dual-GAN framework employing Dilated Residual Inception Blocks (DRIBs). The first GAN translated SAR data into optical imagery, while the second fused this output with a cloud-contaminated optical image to produce a cloud-free result. This design effectively removed cloudy regions while preserving clear areas, demonstrating strong qualitative performance, though quantitative results remained suboptimal.
Ebel et al.~\cite{UnCRtainTS} expanded the research frontier by incorporating uncertainty prediction into multispectral satellite image reconstruction with their model UnCRtainTS, addressing a notable gap in cloud removal research. The model outperformed state-of-the-art approaches at the time in both mono- and multi-temporal scenarios and additionally provided per-pixel uncertainty maps to indicate the reliability of spectral predictions.
Kwak and Park~\cite{assessing_MT_cGANS_s2o_crop} proposed the Multi-Temporal Conditional GAN (MTcGAN) to enhance SAR-to-optical translation for early-stage crop monitoring. The model utilized a SAR–optical pair from a reference date together with a SAR image from a prediction date to capture temporal dynamics, generating more realistic optical outputs. MTcGAN achieved superior visual quality and spectral consistency compared to conventional methods, particularly under conditions of low or rapidly changing crop vitality.

Diffusion models have recently entered this field. Bai et al.~\cite{c_diffusion_s2o} introduced a conditional diffusion model for SAR-to-optical translation. Despite limited experimentation due to resource constraints, the model showed promising potential compared to existing GAN-based methods. Bai et al.~\cite{s2o_color_super_diff} later extended the framework with color supervision, further improving reconstruction fidelity. More recently, Zou et al.~\cite{DiffCR} presented DiffCR, a fast conditional diffusion framework for cloud removal from optical satellite imagery, which currently represents the state-of-the-art on the SEN12-MS-CR~\cite{sen12ms-cr_2021} dataset. Unlike traditional GAN-based or computationally intensive diffusion approaches, DiffCR employs a decoupled conditional architecture and a novel Time and Condition Fusion Block (TCFBlock) to efficiently fuse multiscale spatio-temporal features. By directly predicting clean cloud-free images instead of noise, it achieves faster convergence and remarkable fidelity, producing high-quality reconstructions in as few as 1–5 denoising steps with over 95\% lower computational cost than previous diffusion-based methods.

Following the success of Vision Transformers (ViTs)~\cite{ViT_2020}, transformer-based architectures have also emerged in SAR-to-optical translation. Zhao et al.~\cite{hvt_cgan} proposed the Hybrid Vision Transformer cGAN (HVT-cGAN), combining CNN and ViT branches to capture both local details and global semantics. A Convolutional Attention Fusion Module (CAFM) adaptively merged multiscale features, enhancing texture and color fidelity. Trained on the SEN1-2 dataset, HVT-cGAN achieved superior visual quality and quantitative performance over earlier GAN-based methods. Subsequently, Park et al.~\cite{s2o_ViT_cGAN} extended this direction with a multiscale ViT-based cGAN architecture that integrates perceptual loss and a two-phase transfer learning strategy to improve realism and training stability. Trained on a large subset of the SEN1-2 dataset, the model achieved strong quantitative results, though the visual fidelity of generated images still leaves room for improvement.

Despite the remarkable performance of existing approaches, most studies have focused on a limited subset of optical bands—typically the visible RGB bands (B4, B3, B2), with a few extending to the NIR band (B8). Only a small body of literature has addressed the reconstruction of the full 13-band Sentinel-2 spectrum. For instance, Meraner et al.~\cite{CR_SEN2_dRNN} proposed DSen2-CR, a deep residual neural network for cloud removal in Sentinel-2 imagery that integrates SAR–optical data fusion to enhance reconstruction under thick cloud cover. Built upon the DSen2 super-resolution ResNet, the model introduces a Cloud-Adaptive Regularized Loss (CARL) to preserve uncorrupted input information while reconstructing only clouded regions. Trained and evaluated on the SEN12MS-CR dataset, DSen2-CR demonstrated strong generalization across global scenes and outperformed GAN-based baselines in both spectral and structural fidelity. Across-band evaluation confirmed high reconstruction accuracy over all 13 Sentinel-2 bands, with the best performance on surface bands and slightly higher errors in atmospheric ones.

Building on this foundation, the present thesis validates SAR-to-optical image translation across the full 13 Sentinel-2 spectral bands, including detailed band-wise performance assessment, and further investigates its applicability to cloud removal.
