\chapter{Literature Review}


Cloud contamination in optical remote sensing imagery hinders continuous Earth observation, limiting applications such as crop monitoring and land cover classification. Synthetic Aperture Radar (SAR) systems can penetrate clouds, enabling data acquisition in all weather conditions. This capability makes SAR data valuable for filling gaps in optical time series. Consequently, research has focused on SAR-optical data fusion and SAR-to-optical image translation for cloud removal to improve data availability.

Early cloud removal methods utilized traditional image processing techniques like sparse representation and multi-temporal dictionary learning. For instance, Huang et al. (2015) proposed using SAR data for cloud removal via sparse representation. This approach was subsequently extended by Xu et al. (2016) using multi-temporal dictionary learning. However, these methods exhibited limitations when confronted with extensive cloud coverage or dynamic surface conditions.

The advent of deep learning, particularly Generative Adversarial Networks (GANs), transformed remote sensing image processing for tasks like cloud removal and SAR-to-optical translation. Goodfellow et al. (2014) introduced the foundational GAN framework for generating synthetic data. This was extended by Mirza and Osindero (2014) with Conditional GANs (cGANs), which generate outputs conditioned on specific inputs. These models provide a powerful framework for image-to-image translation tasks central to SAR-to-optical conversion and cloud removal.

Enomoto et al. (2017) applied cGANs to generate cloud-free RGB images from cloudy RGB and near-infrared (NIR) data, though this approach was limited by the opacity of dense clouds to both visible and infrared light. To overcome this, Grohnfeldt et al. (2018) developed the \textbf{SAR-Opt-cGAN} to fuse Sentinel-2 optical and Sentinel-1 SAR imagery, demonstrating superior reconstruction performance by utilising SAR information. Concurrently, Bermudez et al. (2018) investigated SAR-to-optical synthesis using cGANs for crop classification. The development of large-scale datasets, namely \textbf{SEN1-2} (Schmitt et al., 2018) and its successor \textbf{SEN12MS} (Schmitt et al., 2019b), was critical for this field, providing large quantities of co-registered SAR and optical image pairs.

Research in GAN-based methods continued to advance. Fuentes Reyes et al. (2019) analyzed the optimization and limitations of cGANs for SAR-to-optical translation. Wang et al. (2019) introduced a supervised \textbf{CycleGAN} that incorporated cycle-consistency to improve translation performance. Meraner et al. (2020) proposed \textbf{DSen2-CR}, a deep residual network for SAR-optical fusion that demonstrated robustness to extensive cloud cover. Abady et al. (2020) used \textbf{ProGAN} to generate synthetic multispectral images, showing it could preserve spectral relationships. Pan (2020) developed the \textbf{Spatial Attention Generative Adversarial Network (SpA GAN)}, which incorporates a spatial attention mechanism to focus on cloud-affected areas, enhancing information recovery and generating higher quality cloudless images. Gao et al. (2020) also contributed a GAN-based method for cloud removal by fusing high-resolution optical and SAR images.

More recent GAN models have incorporated greater complexity. Naderi Darbaghshahi et al. (2021) proposed a two-GAN model for cloud removal, using a first GAN for SAR-to-optical translation and a second for cloud removal, incorporating \textbf{dilated residual inception blocks (DRIBs)} to increase receptive fields. Ebel et al. (2022) introduced \textbf{UnCRtainTS}, which combines an attention-based architecture with multivariate uncertainty prediction for multi-temporal cloud removal, achieving state-of-the-art results on challenging datasets. Kwak and Park (2024) evaluated \textbf{Multi-Temporal Conditional Generative Adversarial Networks (MTcGANs)} for crop monitoring, highlighting their advantages over conventional methods by allowing diverse input data compositions. Liu et al. (2024) developed \textbf{S2MS-GAN}, which includes a \textbf{TV-BM3D module} for speckle noise reduction and spectral attention to enhance spectral features in high-resolution SAR-to-multispectral translation.

Diffusion Models have also been applied to these tasks. Bai et al. (2023) proposed a conditional diffusion model for SAR-to-optical translation, using the SAR image as a constraint during the sampling process to produce high-quality optical images with clearer borders, aiding interpretation. A subsequent work introduced a \textbf{color-supervised diffusion model} to address color shift issues in the generated images. Zou et al. (2023) presented \textbf{DiffCR}, a fast conditional diffusion framework for cloud removal that uses a decoupled encoder and an efficient fusion block to achieve state-of-the-art performance with high computational efficiency.

Vision Transformer (ViT) architectures, known for their ability to capture global image relationships, have also been adapted for SAR-to-optical translation. Dosovitskiy et al. (2020) first demonstrated the power of transformers for image recognition, establishing a new paradigm for vision tasks. In the context of remote sensing, Park et al. (2025) incorporated \textbf{multiscale Vision Transformer (ViT) blocks} into a cGAN, leveraging a ViT processor to extract multi-level terrain and color features, which improved the detail and colorization of generated optical images. This successful integration highlights the potential of transformer-based architectures for extracting the complex, large-scale features inherent in satellite imagery.

While ViTs have proven effective, their quadratic complexity in relation to image size presents computational challenges, particularly for high-resolution remote sensing data. A more recent development, the Mamba architecture, introduced by Gu \& Dao (2023), addresses this limitation. Mamba and its vision-centric variants are based on State Space Models (SSMs) that can model long-range dependencies with linear complexity. This efficiency has led to their rapid adoption in various remote sensing tasks. For instance, recent studies have demonstrated the success of Vision Mamba in demanding applications such as semantic segmentation, change detection, and even in related optical-to-optical translation tasks.

The proven ability of SSMs to effectively model complex spatial data in other remote sensing applications, combined with their significant computational advantages over ViTs, makes them a highly promising candidate for the more challenging cross-modality problem of SAR-to-optical translation. However, the application of Vision Mamba to this specific task, which requires bridging the gap between active microwave and passive optical data, remains an open research question. This thesis aims to address this gap by implementing and evaluating a Vision Mamba-based model for generating optical Sentinel-2 data from Sentinel-1 SAR imagery.



