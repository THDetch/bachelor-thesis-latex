\chapter{Literature Review}
Cloud contamination in optical remote sensing imagery hinders continuous Earth observation, limiting applications such as crop monitoring and land cover classification. Synthetic Aperture Radar (SAR) systems can penetrate clouds, enabling data acquisition in all weather conditions. This capability makes SAR data valuable for filling gaps in optical time series, driving research into SAR–optical fusion and SAR-to-optical image translation for cloud removal.

Early methods leveraged traditional signal processing techniques. Huang et al.~\cite{huang2015} introduced sparse representation-based cloud removal using SAR data, which Xu et al.~\cite{xu2016} extended via multi-temporal dictionary learning. These approaches, however, struggled under heavy cloud cover or highly dynamic surface changes.

Deep learning transformed the field. The foundational GAN framework was introduced by Goodfellow et al.~\cite{goodfellow2014}, and Mirza \& Osindero~\cite{mirza2014} extended it to conditional GANs (cGANs), ideal for image-to-image tasks like SAR-to-optical translation. Enomoto et al.~\cite{enomoto2017} applied cGANs for cloud removal using NIR input, though dense clouds remained problematic. To address this, Grohnfeldt et al.~\cite{grohnfeldt2018} proposed SAR-Opt-cGAN to fuse Sentinel-1 SAR and Sentinel-2 optical data; Bermudez et al.~\cite{bermudez2018} explored cGAN-based SAR-to-optical synthesis for crop classification. The SEN1-2~\cite{schmitt2018} and SEN12MS~\cite{schmitt2019} datasets were pivotal for training deep models.

Advancements continued with Fuentes Reyes et al.~\cite{fuentes2019} on cGAN optimization, Wang et al.~\cite{wang2019} with supervised CycleGANs for translation, Meraner et al.~\cite{meraner2020} introducing DSen2-CR networks, Abady et al.~\cite{abady2020} leveraging ProGANs, and Pan~\cite{pan2020} with spatial-attention models. Gao et al.~\cite{gao2020} developed fusion-based GAN approaches for high-resolution images.

Recent models show increasing complexity: Naderi Darbaghshahi et al.~\cite{naderi2021} proposed a two-GAN model with DRIBs, Ebel et al.~\cite{ebel2022} introduced UnCRtainTS with uncertainty prediction, Kwak \& Park~\cite{kwak2024} proposed MTcGANs, and Liu et al.~\cite{liu2024} developed S2MS-GAN.

Diffusion models have found their way into this field: Bai et al.~\cite{bai2023} proposed a conditional diffusion model; Bai et al.~\cite{bai2024} extended it with color supervision; Zou et al.~\cite{zou2023} introduced the efficient DiffCR framework.

Vision Transformers (ViT) also made inroads: Dosovitskiy et al.~\cite{dosovitskiy2020} introduced the original ViT, while Park et al.~\cite{park2025} integrated multiscale ViT blocks into a cGAN for SAR-optical tasks. However, ViTs are computationally intensive.

Alternatives emerged via SSMs: Gu \& Dao~\cite{gu2023} introduced Mamba—an efficient, linear-complexity sequence model. U-Mamba~\cite{umamba2024} adapted this into a U-Net architecture, and Swin-UMamba~\cite{swinumamba2024} enhanced it further using ImageNet pretraining. Swin-UNet~\cite{swinunet2023} remains a benchmark transformer-based segmentation model. 
